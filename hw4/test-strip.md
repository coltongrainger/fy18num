{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods\n",
    "\n",
    "To solve the linear system given $A\\mathbf{x} = \\mathbf{b}$, we'll implement three iterative methods: Jacobi, Gauss-Seidel, and Successive Over Relaxation (SOR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[ 4, -1,  0], \n",
    "              [-1,  4, -1], \n",
    "              [ 0, -1,  4]])\n",
    "b = np.array([ 2,  4, 10])\n",
    "x0 = np.zeros(3)\n",
    "eps = 10**-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will:\n",
    "\n",
    "- Examine whether the sequence of vectors generated by each method's fixed point iteration function does converge to the analytic solution $\\mathbf{x} = (1,2,3)^T$. \n",
    "- Take $\\mathbf{x}_0 = (0,0,0)^T$ as our initial term and proceed until the $l_\\infty$ norm of the difference between two successive approximations is less than $\\varepsilon = 10^{-7}$.\n",
    "- Time the convergence of each method with python's ``%%timeit`` magic function.\n",
    "\n",
    "### Jacobi method \n",
    "\n",
    "This method simultaneously computes all entries in the output vector with matrix multiplication.\n",
    "\n",
    "The function `jacobi` takes as input\n",
    "\n",
    "- `A`, a 2-D square array\n",
    "- `b` and `x0`, 1-D arrays\n",
    "\n",
    "and outputs (the tuple)\n",
    "\n",
    "- `x1`, a 1-D array that succeeds `x0` in approximating `b`\n",
    "- `np.linalg.norm(x1 - x0, ord=np.inf)`, the $l_\\infty$ norm of `x1-x0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "def jacobi(A,b,x0, *args):\n",
    "    U = np.triu(A,1)\n",
    "    D = np.diag(A)\n",
    "    L = np.tril(A,-1)\n",
    "    x1 = (1/D) * (b - np.matmul((L+U),x0))\n",
    "    return x1, np.linalg.norm(x1 - x0, ord=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now iterate changes to the state of `x_approx` by looping calls to the `jacobi` function while `interstep` is greater than $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_approx = x0\n",
    "interstep = eps+1 #must dominate eps to enter while loop\n",
    "while interstep > eps:\n",
    "    x_approx,interstep = jacobi(A,b,x_approx)\n",
    "x_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we time the execution of the entire cell, and take the average of the 3 best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 70.1 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x_approx = x0\n",
    "interstep = eps+1\n",
    "while interstep > eps:\n",
    "    x_approx,interstep = jacobi(A,b,x_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Seidel method\n",
    "\n",
    "In contrast to the Jacobi method, the Gauss-Seidel method computes each entry `x1[i]` of the output vector in succession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "def gauss_seidel(A,b,x0, *args):\n",
    "    n = len(x0)\n",
    "    x1=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        new = np.sum(A[i,:i] * x1[:i])\n",
    "        old = np.sum(A[i,i+1:] * x0[i+1:])\n",
    "        x1[i] = (1 / A[i,i]) * (b[i] - new - old)\n",
    "    return x1, np.linalg.norm(x1 - x0, ord=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update `x_approx` in a similar a while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_approx = x0\n",
    "interstep = eps+1\n",
    "while interstep > eps:\n",
    "    x_approx,interstep = gauss_seidel(A,b,x_approx)\n",
    "x_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 97.2 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x_approx = x0\n",
    "interstep = eps+1\n",
    "while interstep > eps:\n",
    "    x_approx,interstep = gauss_seidel(A,b,x_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Successive over relaxation\n",
    "\n",
    "Successive over relaxation generalizes the Gauss-Seidel method, and as such the `SOR` function requires an additional input `omega`. \n",
    "\n",
    "We choose the optimal parameter $\\omega = 2/(1+\\sqrt{7/8})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def SOR(A,b,x0,omega):\n",
    "    n = len(x0)\n",
    "    x1=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        jaterm = (1-omega) * x0[i]\n",
    "        gsnew = np.sum(A[i,:i] * x1[:i])\n",
    "        gsold = np.sum(A[i,i+1:] * x0[i+1:])\n",
    "        gsterm = (omega / A[i,i]) * (b[i] - gsnew - gsold)\n",
    "        x1[i] = jaterm + gsterm\n",
    "    return x1, np.linalg.norm(x1-x0, ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "omega = 2/(1+np.sqrt(7/8))\n",
    "x_approx = x0\n",
    "interstep = eps+1\n",
    "while interstep > eps:\n",
    "    x_approx,interstep = SOR(A,b,x_approx,omega)\n",
    "x_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x_approx = x0\n",
    "interstep = eps+1\n",
    "while interstep > eps:\n",
    "    x_approx,interstep = SOR(A,b,x_approx,omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis\n",
    "\n",
    "The function `capture_error` logs the $l_\\infty$ norm of the error as each `method` runs until the desired accuracy is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_error(method,A,b,x_approx,omega,x_exact,eps):\n",
    "    error = np.array([np.linalg.norm(x_approx - x_exact, ord=np.inf)])\n",
    "    while error[-1] > eps:\n",
    "        x_approx = method(A,b,x_approx,omega)[0]\n",
    "        error = np.append(error,\\\n",
    "                np.linalg.norm(x_approx - x_exact, ord=np.inf))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot each method's error by iteration, and summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "ax = plt.figure().gca()\n",
    "\n",
    "plt.title('Prob 1(b): $l_\\infty$ norm of error by iteration')\n",
    "x_exact = np.array([1,2,3])\n",
    "for method in [jacobi,gauss_seidel,SOR]:\n",
    "    ax.semilogy(capture_error(method,A,b,x0,omega,x_exact,eps),\\\n",
    "        label=method.__name__)\n",
    "    \n",
    "plt.legend()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "# to estimate the asymptotic error constant\n",
    "def est_asym_error(method,A,b,x0,omega,eps,k):\n",
    "    interstep = np.array([eps+1])\n",
    "    while interstep[-1] > eps:\n",
    "        x1 = method(A,b,x0,omega)[0]\n",
    "        interstep = np.append(interstep,\\\n",
    "                np.linalg.norm(x0 - x1, ord=np.inf))\n",
    "        x0 = x1\n",
    "    C = interstep[k+1]/interstep[k]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    }
   },
   "outputs": [],
   "source": [
    "[est_asym_error(method,A,b,x0,omega,eps,6)\\\n",
    "    for method in [jacobi,gauss_seidel,SOR]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method | iterations | runtime | error const.\n",
    "--- | --- | --- | ---\n",
    "jacobi | 17 |\n",
    "1130 µs | 0.25\n",
    "gauss_seidel | 10 | 907 µs | 0.125\n",
    "SOR | 7 | 791 µs | 0.0489\n",
    "\n",
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x,y = sym.symbols('x y')\n",
    "F = sym.Matrix([x**2 + y**2 - 4, x**2 - y**2 - 1])\n",
    "print(\"F    =\", F)\n",
    "\n",
    "J = F.jacobian(sym.Matrix([x,y]))\n",
    "print(\"J    =\", J)\n",
    "\n",
    "init = [(x,1),(y,1)]\n",
    "vec0 = sym.Matrix([coord[1] for coord in init])\n",
    "print(\"vec0 =\", vec0)\n",
    "\n",
    "h = J.subs(init).LUsolve(-F.subs(init))\n",
    "print(\"h    =\", h)\n",
    "\n",
    "vec1 = h + vec0\n",
    "print(\"vec1 =\", vec1)"
